{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower().strip())\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"HTTPURL\", str(text), flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.MULTILINE)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As parts of the data where not in English, we checked for other languages in order to exclude e.g. French or German text.\n",
    "#we found about 200 entries with mixed langugages.\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang(text):\n",
    "  doc = nlp(text)\n",
    "  return doc._.language['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy[cuda112]\n",
    "gpu = spacy.prefer_gpu()\n",
    "print('GPU:', gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['lang'] = complete_data['text'].apply(lambda x: get_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textacy\n",
    "\n",
    "def normalize_text(text):\n",
    "    # remove emojis\n",
    "    text = textacy.preprocess.replace_emojis(text, '')\n",
    "    # remove unicode characters\n",
    "    text = textacy.preprocess.replace_unicode(text, '')\n",
    "    # remove noisy characters\n",
    "    text = textacy.preprocess.remove_punct(text, marks='•')\n",
    "    text = textacy.preprocess.replace_hyphens(text, '-')\n",
    "    text = textacy.preprocess.replace_quotes(text, '\"')\n",
    "    # remove URLs\n",
    "    text = textacy.preprocess.replace_urls(text, 'HTTPURL')\n",
    "    # remove hashtags\n",
    "    text = textacy.preprocess.replace_hashtags(text, '')\n",
    "    # remove email addresses\n",
    "    text = textacy.preprocess.replace_emails(text, 'EMAIL')\n",
    "    # remove HTML tags\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Preprocessing Steps\n",
    "- splitting the longer text into pieces\n",
    "- for Roberta-large, we removed: \n",
    "  *   emojis\n",
    "  *   unicode\n",
    "  *   'noisy' characters (bulletpoints, hypenated words quotes)\n",
    "  *   urls\n",
    "  *   hashtags\n",
    "  *   email\n",
    "  *   html tags\n",
    "  * this was based on textacy: https://textacy.readthedocs.io/en/latest/api_reference/preprocessing.html\n",
    "  * our model did not improve with those preprocessing steps, so they are not part of the final model\n",
    "* We removed a list of signs and symbols: \n",
    "{'\"',\n",
    " '#',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/',\n",
    " ':',\n",
    " ';',\n",
    " '=',\n",
    " '?',\n",
    " '[',\n",
    " ']',\n",
    " '_',\n",
    " '°',\n",
    " '±',\n",
    " 'é',\n",
    " 'č',\n",
    " 'š',\n",
    " '\\u200b',\n",
    " '‐',\n",
    " '–',\n",
    " '—',\n",
    " ''',\n",
    " ''',\n",
    " '\"',\n",
    " '\"',\n",
    " '•'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
