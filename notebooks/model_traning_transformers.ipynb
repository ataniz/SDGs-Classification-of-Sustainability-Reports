{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install flair\n",
    "!pip install GPUtil\n",
    "!pip install spacy-langdetect\n",
    "# !python -m spacy download en_core_web_trf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower().strip())\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"HTTPURL\", str(text), flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.MULTILINE)\n",
    "    return text.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install flair\n",
    "# !pip install GPUtil\n",
    "# !pip install spacy-langdetect\n",
    "# !python -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart runtime until you get a P100\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean GPU Memory\n",
    "# import torch\n",
    "# from GPUtil import showUtilization as gpu_usage\n",
    "# from numba import cuda\n",
    "\n",
    "\n",
    "# def free_gpu_cache():\n",
    "#     print(\"Initial GPU Usage\")\n",
    "#     gpu_usage()                             \n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     cuda.select_device(0)\n",
    "#     cuda.close()\n",
    "#     cuda.select_device(0)\n",
    "\n",
    "#     print(\"GPU Usage after emptying the cache\")\n",
    "#     gpu_usage()\n",
    "\n",
    "# free_gpu_cache()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "conllu.TokenList = conllu.models.TokenList\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from flair.data import Corpus\n",
    "# from flair.datasets import TREC_6\n",
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/final_models'\n",
    "model_name='roberta-large'\n",
    "frac = 1\n",
    "# # ernie, roberta-large, t5, bert, xlnet\n",
    "# dataset = pd.read_csv('/content/drive/MyDrive/final_models/final_data.csv').sample(frac=1)\n",
    "# dataset = dataset[['sdg', 'text']].rename(columns={'sdg': 'label'}).sample(frac=1)\n",
    "\n",
    "\n",
    "# test = dataset.iloc[0:int(len(dataset)*0.1)]\n",
    "# test.to_csv('/content/drive/MyDrive/final_models/final_test.csv')\n",
    "\n",
    "# train_and_dev = dataset.iloc[int(len(dataset)*0.1):int(len(dataset))]\n",
    "# train_and_dev.to_csv('/content/drive/MyDrive/final_models/train_and_dev.csv')\n",
    "\n",
    "train_and_dev= pd.read_csv('/content/drive/MyDrive/final_models/train_and_dev.csv').sample(frac=frac)\n",
    "train_and_dev = train_and_dev[['label', 'text']]\n",
    "# train_and_dev['text'] = train_and_dev['text'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "train = train_and_dev.iloc[:int(len(train_and_dev)*(8/9))]\n",
    "dev = train_and_dev.iloc[int(len(train_and_dev)*(8/9)):len(train_and_dev)]\n",
    "\n",
    "test = pd.read_csv('/content/drive/MyDrive/final_models/final_test.csv').sample(frac=frac)\n",
    "test = test[['label', 'text']]\n",
    "\n",
    "\n",
    "# # test['text'] = test['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# # # train.to_csv(path+'train.csv')\n",
    "\n",
    "# # # dev.to_csv(path+'dev.csv')\n",
    "\n",
    "train['label'] = '__label__' + train['label'].astype(str)\n",
    "test['label'] = '__label__' + test['label'].astype(str)\n",
    "dev['label'] = '__label__' + dev['label'].astype(str)\n",
    "\n",
    "train.to_csv(f'{path}/{model_name}/train.txt', sep='\\t', index=False, header=False)\n",
    "test.to_csv(f'{path}/{model_name}/test.txt', sep='\\t', index=False, header=False)\n",
    "dev.to_csv(f'{path}/{model_name}/dev.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "# # # del dataset\n",
    "# del train_and_dev\n",
    "# del train\n",
    "# del test\n",
    "# del dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = 'SDG'\n",
    "\n",
    "corpus: Corpus = ClassificationCorpus(f'{path}/{model_name}/',\n",
    "                                      test_file='test.txt',\n",
    "                                      dev_file='dev.txt',\n",
    "                                      train_file='train.txt',\n",
    "                                      label_type=label_type,\n",
    "                                      )\n",
    "\n",
    "\n",
    "document_embeddings = TransformerDocumentEmbeddings('roberta-large', fine_tune=True)\n",
    "#document_embeddings = TransformerDocumentEmbeddings('roberta-large', fine_tune=True, dropout=0.3)\n",
    "\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(label_type=label_type\n",
    "), label_type=label_type, multi_label=False)\n",
    "\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "\n",
    "trainer.fine_tune(f'{path}/{model_name}/',\n",
    "              learning_rate=1e-5,\n",
    "              mini_batch_size=4,\n",
    "              max_epochs=10,\n",
    "              embeddings_storage_mode='none',\n",
    "              write_weights=True\n",
    "              )\n",
    "\n",
    "#learning_rate=1e-6, 3e-6, 2e-5, 3e-5, 4e-5, 5e-5\n",
    "#mini_batch_size=8\n",
    "#max_epoch= 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting our training curves for visualization\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves(f'{path}/{model_name}/loss.tsv')\n",
    "plotter.plot_weights(f'{path}/{model_name}/weights.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 - Hyperparameter Tuning\n",
    "Hyperparameters were defined and tested automatically in order to find the best combination for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from flair.hyperparameter.param_selection import SearchSpace, Parameter\n",
    "\n",
    "search_space = SearchSpace()\n",
    "\n",
    "# define training hyperparameters\n",
    "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[1e-5, 3e-5, 1e-6, 3e-6 ])\n",
    "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[4, 8])\n",
    "search_space.add(Parameter.DROPOUT, hp.uniform, low=0.0, high=0.7)\n",
    "\n",
    "# define transformer embedding hyperparameters\n",
    "search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=['roberta-large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.hyperparameter.param_selection import TextClassifierParamSelector, OptimizationValue\n",
    "\n",
    "# what label do we want to predict?\n",
    "label_type = 'SDG'\n",
    "\n",
    "# create the parameter selector\n",
    "param_selector = TextClassifierParamSelector(\n",
    "    corpus,\n",
    "    label_type,\n",
    "    False,\n",
    "    'resources/results',\n",
    "    max_epochs=2,\n",
    "    fine_tune=True,\n",
    "    training_runs=3,\n",
    "    optimization_value=OptimizationValue.DEV_SCORE\n",
    ")\n",
    "#training_runs=1, 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
